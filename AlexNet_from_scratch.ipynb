{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488c5e99",
   "metadata": {},
   "source": [
    "## 1. Installing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e7356",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "1. When this transformation is applied to an image tensor, it first subtracts the mean value of each color channel from the corresponding pixel values, and then divides the result by the standard deviation of each color channel.\n",
    "\n",
    "2. `shuffle` parameter is set to True to ensure that the images are returned in a random order for each epoch of training, which is useful for reducing overfitting and improving the generalization of the model\n",
    "\n",
    "3. `train_idx` contains the indices of the samples in the training subset, and `valid_idx` contains the indices of the samples in the validation subset. `SubsetRandomSampler` is a PyTorch sampler that selects a random subset of samples from a dataset using a given set of indices. `train_sampler` and `valid_sampler` are PyTorch samplers that select random samples from the training and validation subsets, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913c536",
   "metadata": {},
   "source": [
    "## 2. Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80bc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_loader(data_dir,\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "    \n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4), # to introduce some variability into the training data \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        \n",
    "    # loading the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "    root=data_dir, train=True,\n",
    "    download=True, transform=train_transform,\n",
    "    )\n",
    "    \n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "    root=data_dir, train=True,\n",
    "    download=True, transform=valid_transform,\n",
    "    )\n",
    "    \n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train)) # to get the index of the last sample in the training subset\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "def get_test_loader(data_dir,\n",
    "                   batch_size,\n",
    "                   shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "    \n",
    "    # define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227,227)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,  \n",
    "    ])\n",
    "    \n",
    "    # loading the test data\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir = './data',\n",
    "                                                    batch_size = 64,\n",
    "                                                    augment = False,\n",
    "                                                    random_seed = 1)\n",
    "\n",
    "test_loader = get_test_loader(data_dir = './data',\n",
    "                              batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac5d95",
   "metadata": {},
   "source": [
    "## 3. Creating the AlexNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102522aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, total_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, total_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0),-1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b899294",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "1. The `lr` parameter specifies the learning rate, which determines the step size of the optimizer in updating the parameters. The `weight_decay` parameter is a form of L2 regularization, which penalizes large weights to prevent overfitting. The `momentum` parameter adds a fraction of the previous update to the current update, which helps to stabilize the optimization process and accelerate convergence.\n",
    "\n",
    "2. An epoch is a complete iteration through the entire training set.\n",
    "\n",
    "3. The `predicted == labels` comparison returns a boolean tensor with the same shape as the labels tensor, where `True` indicates a correct prediction and `False` indicates an incorrect prediction. The `sum()` method sums up the number of True values, and the `item()` method converts the resulting PyTorch scalar tensor to a Python integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d7061",
   "metadata": {},
   "source": [
    "## 4. Setting the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classes = 10\n",
    "total_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet(total_classes).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            weight_decay=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec6a04",
   "metadata": {},
   "source": [
    "## 5. Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534785ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader) # getting the total number of batches\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  # looping over the batches\n",
    "        images = images.to(device) # moving both the images and labels to the device\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(images) # predicted outputs\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad() # setting all the gradients to zero before computing gradients for the next batch\n",
    "        loss.backward()\n",
    "        optimizer.step() # uppdating the model's parameters using the computed gradients\n",
    "    \n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, total_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "    # for validation\n",
    "    with torch.no_grad(): # reduces memory usage and speeds up computation, since gradients don't need to be computed during validation\n",
    "        correct = 0 # correctly classified images\n",
    "        total = 0 # total number of images seen during validation\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1) # discarding maximum value in the predicted output tensor and the class with the highest probability\n",
    "            total += labels.size(0) # updating the total number of images seen during validation\n",
    "            \n",
    "            # the number of correctly classified images\n",
    "            correct += (predicted == labels).sum().item() \n",
    "            \n",
    "            # deletes the variables containing the images, labels, and predicted outputs tensors, to free up memory.\n",
    "            del images, labels, outputs\n",
    "            \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06cccb0",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
